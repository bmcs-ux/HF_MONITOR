
import pandas as pd
import numbers
from datetime import datetime, timedelta
import warnings
import numpy as np
import time
from io import StringIO
import re
import pickle
from typing import Optional, Dict, Any, Tuple
from scipy.stats import norm
import sys
import os
import requests

# NEW: Using MT5Adapter
from mt5_adapter import MT5Adapter
from mt5_adapter import MT5_TIMEFRAME_MAP

# Global helper function to convert NumPy floats to native Python types
def convert_numpy_floats(obj):
    if isinstance(obj, np.floating):
        if np.isnan(obj) or np.isinf(obj):
            return None
        return float(obj)
    elif isinstance(obj, numbers.Number):
        if isinstance(obj, float) and (np.isnan(obj) or np.isinf(obj)):
            return None
        return obj
    elif isinstance(obj, (str, bool, type(None))):
        return obj
    elif isinstance(obj, dict):
        return {k: convert_numpy_floats(v) for k, v in obj.items()}
    elif isinstance(obj, (list, tuple)):
        return [convert_numpy_floats(elem) for elem in obj]
    try:
        return float(obj)
    except (ValueError, TypeError):
        return obj


current_script_dir = "/content/drive/MyDrive/books/VARX_REGRESION"
if current_script_dir not in sys.path:
    sys.path.insert(0, current_script_dir)
import vps_colab_connector

VPS_PARAM_DIR = current_script_dir
VPS_DATA_DIR = current_script_dir

if VPS_PARAM_DIR not in sys.path:
    sys.path.insert(0, VPS_PARAM_DIR)

import parameter

MT5_LOGIN = Your_MT5_username
MT5_PASSWORD = "YOUR_MT5_PASSWORD"
MT5_SERVER = "YOUR_MT5_SERVER"

preprocessing_path = os.path.join(current_script_dir, 'preprocesing')
if preprocessing_path not in sys.path:
    sys.path.insert(0, preprocessing_path)

from preprocesing.log_return import apply_log_return_to_price as _apply_log_return_to_price
from preprocesing.combine_data import combine_log_returns as _combine_log_returns
from preprocesing.stationarity_test import test_and_stationarize_data as _test_and_stationarize_data

warnings.filterwarnings("ignore")

COLAB_API_KEY_FOR_MONITOR = "YOUR_API_KEY"
COLAB_URL_FILE_PATH = os.path.join(VPS_DATA_DIR, "colab_ngrok_url.txt")

TRADE_ENGINE_API_URL = "http://127.0.0.1:8081/receive_signal"

def format_for_dashboard(rls_forecasts, latest_prices):
    """
    Menyederhanakan data RLS agar langsung bisa dibaca oleh JavaScript Dashboard.
    Mengubah format nested menjadi flat (angka murni).
    """
    formatted_forecast = {}
    for sym, data in rls_forecasts.items():
        # Ambil hanya harga prediksi (float)
        if isinstance(data, dict) and 'rls_predicted_price' in data:
            formatted_forecast[sym] = data['rls_predicted_price']
        else:
            formatted_forecast[sym] = data # Fallback jika sudah berupa angka

    return formatted_forecast

def _build_regressor_matrix(log_stream, current_hf_combined_log_returns_df, latest_hf_fred_exog_df, lagged_hf_log_returns_df, maxlags, endog_names_group, exog_names_group):
    phi_list = [1.0]

    # 1. Endogenous Lags
    for lag in range(1, maxlags + 1):
        for endog_name in endog_names_group:
            col_name = f'Lag{lag}_{endog_name}'
            val = 0.0
            if col_name in lagged_hf_log_returns_df.columns:
                # Gunakan .iloc[0] untuk mengambil baris pertama tanpa peduli label indeks
                val = lagged_hf_log_returns_df[col_name].iloc[0]
            else:
                #log_stream.write(f"    [WARN] Lagged column {col_name} not found. Using 0.\n")
                val = 0.0
            phi_list.append(val if pd.notnull(val) else 0.0)

    # 2. Exogenous Variables
    for exog_name in exog_names_group:
        val = 0.0
        # Cek di FRED Exog
        if exog_name in latest_hf_fred_exog_df.columns:
            # PERBAIKAN DISINI: Gunakan .iloc[0] untuk menghindari KeyError pada DatetimeIndex
            val = latest_hf_fred_exog_df[exog_name].iloc[0]
            
        elif exog_name in current_hf_combined_log_returns_df.columns:
            val = current_hf_combined_log_returns_df[exog_name].iloc[0]
        else:
            log_stream.write(f"    [WARN] Exogenous column {exog_name} not found. Using 0.\n")
            
        phi_list.append(val if pd.notnull(val) else 0.0)

    return np.array([phi_list])

def _perform_rls_update(log_stream, theta: np.ndarray, P: np.ndarray, Phi: np.ndarray, Y_t: np.ndarray, forgetting_factor: float) -> Tuple[np.ndarray, np.ndarray]:
    if Y_t.ndim == 1:
        Y_t = Y_t.reshape(1, -1)

    predicted_Y = Phi @ theta
    e = Y_t - predicted_Y

    K_scalar_denom = P @ Phi.T / (forgetting_factor + Phi @ P @ Phi.T)

    updated_theta = theta + K_scalar_denom @ e

    updated_P = (1.0 / forgetting_factor) * (P - K_scalar_denom @ Phi @ P)

    return updated_theta, updated_P

def _extract_baseline_varx_params(log_stream, fitted_model_obj, n_endog_group, k_regressors, endog_names_group, exog_names_group, maxlags) -> Optional[np.ndarray]:
    try:
        model_params = fitted_model_obj.params
        param_names = fitted_model_obj.param_names

        log_stream.write(f"    [INFO] Expected k_regressors for RLS theta: {k_regressors}\n")

        theta_ref = np.zeros((k_regressors, n_endog_group))

        for i, target_endog_name in enumerate(endog_names_group):
            regressor_idx_in_phi = 0

            const_param_name = 'intercept' if n_endog_group == 1 else f'intercept.{target_endog_name}'
            if const_param_name in param_names:
                theta_ref[regressor_idx_in_phi, i] = model_params[param_names.index(const_param_name)]
            else:
                log_stream.write(f"    [WARN] Constant '{const_param_name}' not found for {target_endog_name}. Assuming 0.\n")
            regressor_idx_in_phi += 1

            for lag in range(1, maxlags + 1):
                for source_endog_name in endog_names_group:
                    if n_endog_group > 1:
                        lagged_param_name = f'L{lag}.{source_endog_name}.{target_endog_name}'
                    else:
                        lagged_param_name = f'L{lag}.{source_endog_name}'

                    if lagged_param_name in param_names:
                        theta_ref[regressor_idx_in_phi, i] = model_params[param_names.index(lagged_param_name)]
                    else:
                        log_stream.write(f"    [WARN] Lagged param '{lagged_param_name}' not found for {target_endog_name}. Assuming 0.\n")
                    regressor_idx_in_phi += 1

            for exog_name in exog_names_group:
                if n_endog_group > 1:
                    exog_param_name = f'beta.{exog_name}.{target_endog_name}'
                else:
                    exog_param_name = exog_name

                if exog_param_name in param_names:
                    theta_ref[regressor_idx_in_phi, i] = model_params[param_names.index(exog_param_name)]
                else:
                    log_stream.write(f"    [WARN] Exog param '{exog_param_name}' not found for {target_endog_name}. Assuming 0.\n")
                regressor_idx_in_phi += 1

        log_stream.write(f"    [OK] Baseline VARX parameters extracted successfully. Shape: {theta_ref.shape}\n")
        return theta_ref

    except Exception as e:
        log_stream.write(f"    [ERROR] Failed to extract baseline VARX parameters: {e}\n")
        return None

def rls_forecast_step(
    log_stream,
    theta_rls: np.ndarray,
    current_hf_combined_log_returns_df,
    latest_hf_fred_exog_df,
    lagged_hf_log_returns_df,
    maxlags: int,
    endog_names_group: list,
    exog_names_group: list
) -> np.ndarray:
    """
    Perform 1-step-ahead forecast using RLS-adapted parameters.

    Returns
    -------
    Y_hat : np.ndarray
        Shape (1, n_endog_group)
    """

    # Build regressor Φ_t
    Phi_t = _build_regressor_matrix(
        log_stream=log_stream,
        current_hf_combined_log_returns_df=current_hf_combined_log_returns_df,
        latest_hf_fred_exog_df=latest_hf_fred_exog_df,
        lagged_hf_log_returns_df=lagged_hf_log_returns_df,
        maxlags=maxlags,
        endog_names_group=endog_names_group,
        exog_names_group=exog_names_group
    )

    # Sanity checks
    if Phi_t.shape[1] != theta_rls.shape[0]:
        raise ValueError(
            f"Regressor size mismatch: Phi_t has {Phi_t.shape[1]} cols, "
            f"theta has {theta_rls.shape[0]}"
        )

    # Forecast
    Y_hat = Phi_t @ theta_rls  # (1 × k) @ (k × n_endog)

    log_stream.write(
        f"    [FORECAST] RLS-based forecast computed | Shape: {Y_hat.shape}\n"
    )

    return Y_hat

def infer_rls_expected_return(
    log_stream,
    pair_name,
    rls_estimators,
    current_hf_combined_log_returns_df,
    latest_hf_fred_exog_df,
    lagged_hf_log_returns_df
):
    """
    Mencari grup RLS secara otomatis berdasarkan pair_name dan menghitung expected return.
    """
    target_group = None
    target_idx = -1

    # 1. Proses Pencarian Grup dan Indeks Otomatis
    for g_name, estimator in rls_estimators.items():
        for idx, full_col_name in enumerate(estimator['endog_names']):
            # Mencocokkan "XAU/USD" ke "XAU/USD_Close_Log_Return"
            if pair_name in full_col_name:
                target_group = g_name
                target_idx = idx
                break
        if target_group:
            break

    # 2. Validasi Keberadaan Data
    if not target_group:
        log_stream.write(f"    [WARN] {pair_name} tidak ditemukan di grup RLS manapun. Skipping.\n")
        return None

    # 3. Eksekusi Forecast menggunakan Data dari Estimator yang ditemukan
    try:
        est = rls_estimators[target_group]
        
        Y_hat = rls_forecast_step(
            log_stream=log_stream,
            theta_rls=est["theta"],
            current_hf_combined_log_returns_df=current_hf_combined_log_returns_df,
            latest_hf_fred_exog_df=latest_hf_fred_exog_df,
            lagged_hf_log_returns_df=lagged_hf_log_returns_df,
            maxlags=est["maxlags"],
            endog_names_group=est["endog_names"],
            exog_names_group=est["exog_names"]
        )

        expected_return = float(Y_hat[0, target_idx])

        log_stream.write(
            f"    [INFER] Group: {target_group} | Asset: {pair_name} | "
            f"Exp.Return: {expected_return:+.6f}\n"
        )
        return expected_return

    except Exception as e:
        log_stream.write(f"    [ERROR] Gagal menghitung RLS forecast untuk {pair_name}: {e}\n")
        return None

def fetch_high_frequency_data(log_stream, mt5_adapter_instance, mt5_timeframe_map, PAIRS, HF_LOOKBACK_DAYS, HF_BASE_INTERVAL):
    log_stream.write(f"\n[INFO] Mulai pengunduhan data high-frequency untuk {len(PAIRS)} pasangan dari MetaTrader5...\n")
    hf_data_dfs = {}
    end_date = datetime.now()
    start_date = end_date - timedelta(days=HF_LOOKBACK_DAYS)

    start_ts = int(start_date.timestamp())
    end_ts = int(end_date.timestamp())

    # Get MT5 timeframe constant
    mt5_timeframe = MT5_TIMEFRAME_MAP.get(HF_BASE_INTERVAL)
    if mt5_timeframe is None:
        log_stream.write(f"  [ERROR] Interval '{HF_BASE_INTERVAL}' tidak didukung oleh MetaTrader5. Silakan periksa MT5_TIMEFRAME_MAP.\n")
        return {}

    for pair_name, symbol in PAIRS.items():
        log_stream.write(f"  [INFO] Mengunduh {pair_name} ({symbol}) dengan interval {HF_BASE_INTERVAL} dari {start_date.date()} hingga {end_date.date()}\n")
        try:
            import textwrap

            code = (
                f'mt5.copy_rates_range("{symbol}", {mt5_timeframe}, '
                f'__import__("datetime").datetime.fromtimestamp({start_ts}), '
                f'__import__("datetime").datetime.fromtimestamp({end_ts}))'
            )

            mt5_adaptor = MT5Adapter()
            rates_raw = mt5_adaptor.eval(code)

            import rpyc
            rates = rpyc.utils.classic.obtain(rates_raw)

            if rates is not None and len(rates) > 0:
                data = pd.DataFrame(rates)
                data['time'] = pd.to_datetime(data['time'], unit='s')
                data = data.set_index('time')
                data.index = data.index.tz_localize('UTC')
                data = data[['open', 'high', 'low', 'close', 'tick_volume']]
                data.columns = ['Open', 'High', 'Low', 'Close', 'Volume']
                if 'Volume' not in data.columns:
                    data['Volume'] = 0

                hf_data_dfs[pair_name] = data
                log_stream.write(f"    [OK] Berhasil mengunduh {len(data)} bar untuk {pair_name}. Shape: {data.shape}. Latest Close: {data['Close'].iloc[-1]:.4f} (at {data.index[-1]}).\n")
            else:
                log_stream.write(f"    [WARN] Tidak ada data yang diunduh untuk {pair_name} ({symbol}) dengan interval {HF_BASE_INTERVAL} dalam rentang waktu yang diminta.\n")

        except Exception as e:
            log_stream.write(f"    [ERROR] Gagal mengunduh data untuk {pair_name} ({symbol}) dari MetaTrader5: {e}\n")
            import traceback
            log_stream.write(traceback.format_exc())

    log_stream.write(f"\n[OK] Pengunduhan data high-frequency selesai.\n")
    return hf_data_dfs

def prepare_high_frequency_exogenous_data(log_stream, final_stationarized_fred_data, hf_index):
    log_stream.write(f"\n[INFO] Menyiapkan data eksogen FRED high-frequency...\n")

    if isinstance(final_stationarized_fred_data, dict) and "data" in final_stationarized_fred_data:
        fred_data_to_use = final_stationarized_fred_data["data"]
    else:
        fred_data_to_use = final_stationarized_fred_data

    if not fred_data_to_use:
        log_stream.write(f"  [WARN] Data FRED stasioner kosong. Tidak dapat menyiapkan eksogen high-frequency.\n")
        return pd.DataFrame()

    fred_df_list = []
    for name, df in fred_data_to_use.items():
        if not df.empty:
            value_col = [col for col in df.columns if col.endswith('_FinalTransformed') or col not in ['effective_until_next_release']][0]
            fred_df_list.append(df[[value_col]].rename(columns={value_col: name}))

    if not fred_df_list:
        log_stream.write(f"  [WARN] Tidak ada data nilai yang valid di `final_stationarized_fred_data`. Mengembalikan DataFrame kosong.\n")
        return pd.DataFrame()

    combined_fred_daily = pd.concat(fred_df_list, axis=1, join='outer').sort_index()
    combined_fred_daily = combined_fred_daily.ffill().dropna(how='all')

    if combined_fred_daily.empty:
        log_stream.write(f"  [WARN] Data FRED gabungan kosong setelah pembersihan. Mengembalikan DataFrame kosong.\n")
        return pd.DataFrame()

    if hf_index.tz is None and combined_fred_daily.index.tz is not None:
        combined_fred_daily.index = combined_fred_daily.index.tz_convert(None)
    elif hf_index.tz is not None and combined_fred_daily.index.tz is None:
        combined_fred_daily.index = combined_fred_daily.index.tz_localize(hf_index.tz)
    elif hf_index.tz is not None and combined_fred_daily.index.tz is not None and hf_index.tz != combined_fred_daily.index.tz:
        combined_fred_daily.index = combined_fred_daily.index.tz_convert(hf_index.tz)

    hf_fred_exog_aligned = combined_fred_daily.reindex(hf_index, method='ffill')

    hf_fred_exog_aligned = hf_fred_exog_aligned.dropna(how='all')

    log_stream.write(f"  [OK] Data FRED eksogen high-frequency berhasil disiapkan. Shape: {hf_fred_exog_aligned.shape}\n")
    return hf_fred_exog_aligned

def preprocess_high_frequency_data(log_stream, hf_raw_data_dfs, apply_log_return_to_price_func, combine_log_returns_func, test_and_stationarize_data_func, prepare_high_frequency_exogenous_data_func, final_stationarized_fred_data, alpha):
    log_stream.write(f"\n[INFO] Mulai preprocessing data high-frequency...\n")

    hf_log_returns_raw = apply_log_return_to_price_func(log_stream, hf_raw_data_dfs)
    if hf_log_returns_raw is None:
        hf_log_returns_raw = {}
        log_stream.write(f"[WARN] Gagal menerapkan transformasi log-return pada data high-frequency.\n")

    hf_log_returns_dict = combine_log_returns_func(log_stream, hf_log_returns_raw, return_type='dict')
    hf_combined_log_returns_df = combine_log_returns_func(log_stream, hf_log_returns_raw, return_type='df')

    log_stream.write(f"\n[INFO] Memeriksa stasioneritas data log-return high-frequency...\n")
    hf_stationarity_results, _ = test_and_stationarize_data_func(log_stream, hf_log_returns_dict, {}, alpha)

    hf_fred_exog_aligned = pd.DataFrame()
    if hf_combined_log_returns_df is not None and not hf_combined_log_returns_df.empty:
        hf_fred_exog_aligned = prepare_high_frequency_exogenous_data_func(log_stream, final_stationarized_fred_data, hf_combined_log_returns_df.index)
    else:
        log_stream.write(f"  [WARN] Tidak ada data log-return high-frequency untuk menentukan indeks, melewati penyelarasan FRED.\n")

    log_stream.write(f"\n[OK] Preprocessing data high-frequency selesai.\n")
    return hf_log_returns_dict, hf_combined_log_returns_df, hf_stationarity_results, hf_fred_exog_aligned

def calculate_atr(log_stream, df_ohlc, atr_period=14):
    log_stream.write(f"  [INFO] Calculating ATR ({atr_period}-period)...\n")

    if df_ohlc.empty or not all(col in df_ohlc.columns for col in ['High', 'Low', 'Close']):
        log_stream.write(f"  [WARN] Input DataFrame for ATR calculation is empty or missing 'High', 'Low', 'Close' columns. Returning empty Series.\n")
        return pd.Series(dtype=float)

    high_low = df_ohlc['High'] - df_ohlc['Low']
    high_prev_close = abs(df_ohlc['High'] - df_ohlc['Close'].shift(1))
    low_prev_close = abs(df_ohlc['Low'] - df_ohlc['Close'].shift(1))

    tr = pd.concat([high_low, high_prev_close, low_prev_close], axis=1).max(axis=1)
    tr.name = 'True_Range'

    atr = tr.rolling(window=atr_period).mean()
    atr.name = f'ATR_{atr_period}'

    log_stream.write(f"  [OK] ATR calculation completed for {len(df_ohlc)} observations. Last ATR value: {atr.iloc[-1]:.4f} (at {atr.index[-1]}).\n")
    return atr

def compute_rls_expected_return_for_pair(
    *,
    log_stream,
    pair_name,
    pair_group,
    rls_estimators,
    latest_hf_combined_log_returns_df,
    latest_hf_fred_exog_df_row,
):
    if pair_group not in rls_estimators:
        log_stream.write(
            f"    [WARN] RLS estimator missing for group {pair_group} ({pair_name}).\n"
        )
        return None

    est = rls_estimators[pair_group]

    try:
        return infer_rls_expected_return(
            log_stream=log_stream,
            theta_rls=est["theta"],
            current_hf_combined_log_returns_df=latest_hf_combined_log_returns_df,
            latest_hf_fred_exog_df=latest_hf_fred_exog_df_row,
            lagged_hf_log_returns_df=est["latest_lagged_df"],  #  simpan saat update
            maxlags=est["maxlags"],
            endog_names_group=est["endog_names"],
            exog_names_group=est["exog_names"],
            target_endog_idx=est["pair_target_index"][pair_name],
        )
    except Exception as e:
        log_stream.write(
            f"    [ERROR] Failed computing RLS expected return for {pair_name}: {e}\n"
        )
        return None

def decide_trade(
    log_stream, 
    pair_name, 
    latest_actual_price, 
    expected_return,  # Ini adalah predicted move (log return)
    forecast_std,     # satuan price!
    forecast_std_return, # versi satuan Log retrun dari forecast_std
    hf_atr, 
    equity, 
    risk_pct, 
    k_atr_stop, 
    k_model_stop, 
    snr_threshold, 
    rls_param_deviation_score, 
    rls_deviation_threshold, 
    tp_rr_ratio=1.5
):
    # 1. Hitung Predicted Price (Mean) berdasarkan Log Return
    # Price_next = Price_now * exp(expected_return)
    predicted_mean = latest_actual_price * np.exp(expected_return)

    log_stream.write(f"\n  [INFO] Deciding trade for {pair_name} (Price: {latest_actual_price:.4f}, Pred. Mean: {predicted_mean:.4f})...\n")

    trade_decision = {
        'signal': 'HOLD',
        'entry_price': latest_actual_price,
        'stop_loss': np.nan,
        'take_profit': np.nan,
        'position_units': 0,
        'rr_ratio': np.nan,
        'snr': np.nan,
        'reason': 'No signal generated'
    }

    # 2. Cek Stabilitas RLS
    if rls_param_deviation_score is not None and rls_deviation_threshold is not None:
        if rls_param_deviation_score > rls_deviation_threshold:
            trade_decision['reason'] = f'RLS unstable ({rls_param_deviation_score:.4f} > {rls_deviation_threshold:.4f})'
            log_stream.write(f"    [WARN] {pair_name}: {trade_decision['reason']}\n")
            return trade_decision

    # 3. Validasi Data (Menggunakan forecast_std dari argumen)
    if np.isnan(latest_actual_price) or np.isnan(expected_return) or np.isnan(forecast_std) or forecast_std <= 0:
        trade_decision['reason'] = 'Invalid input data (NaN or non-positive values)'
        log_stream.write(f"    [WARN] {pair_name}: {trade_decision['reason']}\n")
        return trade_decision

    # 4. Dynamic Adjustments
    k_atr_stop_adj = k_atr_stop
    k_model_stop_adj = k_model_stop
    tp_rr_adj = tp_rr_ratio
    snr_thresh_adj = snr_threshold

    if rls_param_deviation_score is not None and not np.isnan(rls_param_deviation_score):
        # Gunakan scaling factor dari parameter global
        inc_factor = 1 + (rls_param_deviation_score * parameter.RLS_SCALING_FACTOR_SL)
        k_atr_stop_adj = min(k_atr_stop * inc_factor, k_atr_stop * parameter.RLS_SL_MAX_MULTIPLIER)
        k_model_stop_adj = min(k_model_stop * inc_factor, k_model_stop * parameter.RLS_SL_MAX_MULTIPLIER)
        
        red_factor = 1 - (rls_param_deviation_score * parameter.RLS_SCALING_FACTOR_TP)
        tp_rr_adj = max(parameter.RLS_TP_RR_MIN, tp_rr_ratio * red_factor)
        
        snr_thresh_adj = snr_threshold * (1 + (rls_param_deviation_score * parameter.RLS_SNR_INCREASE_FACTOR))

    # 5. SNR Calculation (Predicted Log Return / Return Std Dev)
    snr = expected_return / forecast_std_return
    trade_decision['snr'] = snr

    if abs(snr) < snr_thresh_adj:
        trade_decision['reason'] = f'Low SNR ({abs(snr):.2f} < {snr_thresh_adj:.2f})'
        log_stream.write(f"    [INFO] {pair_name}: {trade_decision['reason']}\n")
        return trade_decision

    # 6. SL Distance Calculation
    # Note: forecast_std (price) dikonversi ke price distance
    sl_dist_atr = k_atr_stop_adj * hf_atr
    sl_dist_model = k_model_stop_adj * forecast_std_return * latest_actual_price
    sl_dist = max(sl_dist_atr, sl_dist_model)

    if sl_dist <= 1e-9:
        trade_decision['reason'] = 'SL Distance too small'
        return trade_decision

    # 7. Signal Direction
    if expected_return > 0:
        trade_decision['signal'] = 'BUY'
        direction = 1
    elif expected_return < 0:
        trade_decision['signal'] = 'SELL'
        direction = -1
    else:
        return trade_decision

    sl_dist = abs(sl_dist)
    
    # Hitung Jarak Prediksi (Target Move) dalam satuan Price
    # Jarak dari harga saat ini ke predicted_mean
    prediction_dist = abs(predicted_mean - latest_actual_price)
    
    realized_rr = prediction_dist / sl_dist if sl_dist > 0 else 0
    trade_decision['rr_ratio'] = realized_rr
    # FILTER KRUSIAL: Jika target keuntungan lebih kecil dari risiko, jangan masuk.
    # Batas minimal 0.8 atau 1.0 agar masuk akal setelah spread.
    
    if realized_rr < parameter.RLS_TP_RR_MIN:
        trade_decision['signal'] = 'HOLD'
        trade_decision['reason'] = f'Bad RR Ratio ({realized_rr:.2f} < {parameter.RLS_TP_RR_MIN})'
        log_stream.write(f"    [INFO] {pair_name}: {trade_decision['reason']} (Target: {prediction_dist:.2f}, SL Dist: {sl_dist:.2f})\n")
        return trade_decision

    if direction == 1: # BUY
        # Take Profit sekarang menggunakan nilai Prediksi Mean
        # Kita bisa tambahkan sedikit buffer atau multiplier jika ingin lebih agresif
        tp_price = predicted_mean 
        sl_price = latest_actual_price - sl_dist
    else: # SELL
        tp_price = predicted_mean
        sl_price = latest_actual_price + sl_dist

    # Hitung Realized RR Ratio (Penting untuk log dan monitoring)
    # RR = Jarak TP / Jarak SL
    
    trade_decision['take_profit'] = max(tp_price, 0.000001)
    trade_decision['stop_loss'] = max(sl_price, 0.000001)
    # Position Sizing
    max_risk_usd = equity * risk_pct
    raw_units = max_risk_usd / sl_dist
    
    # Apply caps (Contoh: Max 0.02, Min 0.01)
    position_units = max(0.0, min(raw_units, 0.01))
    if position_units < 0.01:
        position_units = 0.0
        trade_decision['signal'] = 'HOLD'
        trade_decision['reason'] = 'Position size below min (0.01)'
        log_stream.write(f"    [INFO] {pair_name}: {trade_decision['reason']} (Raw: {raw_units:.4f})\n")
        return trade_decision

    trade_decision['position_units'] = round(position_units, 2)
    
    if trade_decision['signal'] != 'HOLD':
        log_stream.write(f"    [OK] {pair_name} {trade_decision['signal']} | SNR: {snr:.2f} | Units: {trade_decision['position_units']}\n")
        trade_decision['reason'] = f"SNR {abs(snr):.2f} > {snr_thresh_adj:.2f}"

    return trade_decision

def detect_price_deviation(log_stream, latest_actual_prices: dict, restored_price_forecasts_with_intervals: dict, confidence_level: float = 0.95):
    log_stream.write(f"\n[INFO] Detecting price deviations from {int(confidence_level*100)}% confidence intervals...\n")
    deviation_results = {}

    ci_lower_col_suffix = f'_Lower_{int(confidence_level*100)}CI'
    ci_upper_col_suffix = f'_Upper_{int(confidence_level*100)}CI'
    mean_col_suffix = '_Mean_Forecast'

    for pair_name, actual_price in latest_actual_prices.items():
        log_stream.write(f"  [INFO] Checking deviation for {pair_name}. Actual price: {actual_price:.4f}\n")
        deviation_info = {
            'ci_breach': False,
            'deviation_metric': np.nan,
            'actual_price': float(actual_price),
            'predicted_mean': np.nan,
            'lower_ci': np.nan,
            'upper_ci': np.nan,
            'forecast_std': np.nan
        }

        if pair_name in restored_price_forecasts_with_intervals:
            forecast_df = restored_price_forecasts_with_intervals[pair_name]
            if forecast_df.empty:
                log_stream.write(f"    [WARN] Forecast DataFrame for {pair_name} is empty. Skipping deviation check.\n")
                deviation_results[pair_name] = deviation_info
                continue

            first_forecast_step = forecast_df.iloc[0]

            mean_forecast_col = f'Close{mean_col_suffix}'
            lower_ci_col = f'Close{ci_lower_col_suffix}'
            upper_ci_col = f'Close{ci_upper_col_suffix}'

            if all(col in first_forecast_step.index for col in [mean_forecast_col, lower_ci_col, upper_ci_col]):
                predicted_mean = float(first_forecast_step[mean_forecast_col])
                lower_ci = float(first_forecast_step[lower_ci_col])
                upper_ci = float(first_forecast_step[upper_ci_col]) 

                deviation_info['predicted_mean'] = predicted_mean
                deviation_info['lower_ci'] = lower_ci
                deviation_info['upper_ci'] = upper_ci

                z_score_value = norm.ppf(1 - (1 - confidence_level) / 2)
                forecast_std = (upper_ci - predicted_mean) / z_score_value if z_score_value != 0 else np.nan
                if np.isnan(forecast_std) or forecast_std <= 0:
                    forecast_std = (predicted_mean - lower_ci) / z_score_value if z_score_value != 0 else np.nan
                deviation_info['forecast_std'] = float(forecast_std)

                log_stream.write(f"    [INFO] Forecasted Mean: {predicted_mean:.4f}, CI: [{lower_ci:.4f}, {upper_ci:.4f}]\n")

                if actual_price < lower_ci:
                    deviation_info['ci_breach'] = True
                    if forecast_std > 0:
                        deviation_info['deviation_metric'] = (actual_price - predicted_mean) / forecast_std
                    log_stream.write(f"    [ALERT] Actual price {actual_price:.4f} is BELOW lower CI {lower_ci:.4f} for {pair_name}. Deviation: {deviation_info['deviation_metric']:.2f} std devs.\n")
                elif actual_price > upper_ci:
                    deviation_info['ci_breach'] = True
                    if forecast_std > 0:
                        deviation_info['deviation_metric'] = (actual_price - predicted_mean) / forecast_std
                    log_stream.write(f"    [ALERT] Actual price {actual_price:.4f} is ABOVE upper CI {upper_ci:.4f} for {pair_name}. Deviation: {deviation_info['deviation_metric']:.2f} std devs.\n")
                else:
                    log_stream.write(f"    [INFO] Actual price {actual_price:.4f} is within CI for {pair_name}.\n")

            else:
                log_stream.write(f"    [WARN] Required forecast columns (Close{mean_col_suffix}, Close{ci_lower_col_suffix}, Close{ci_upper_col_suffix}) not found for {pair_name}. Skipping deviation check.\n")
        else:
            log_stream.write(f"    [WARN] No restored price forecasts found for {pair_name}. Skipping deviation check.\n")

        deviation_results[pair_name] = deviation_info

    log_stream.write(f"\n[OK] Price deviation detection completed.\n")
    return deviation_results

def send_monitoring_data_to_colab(data: dict, log_stream):
    data_converted = convert_numpy_floats(data)

    return vps_colab_connector.send_data_to_colab(
        endpoint="update_monitor_data",
        data=data_converted,
        colab_api_key=COLAB_API_KEY_FOR_MONITOR,
        colab_url_file_path=COLAB_URL_FILE_PATH,
        log_func=lambda msg: log_stream.write(msg + '\n')
    )

def send_signal_to_trade_engine(signal_data: dict, log_stream) -> bool:
    try:
        TE_API_KEY = "bima_12345678"
        headers = {
            "Content-Type": "application/json",
            "X-API-Key": TE_API_KEY
        }
        log_stream.write(f"    [INFO] Sending signal to Trade Engine API: {TRADE_ENGINE_API_URL} with API Key: {TE_API_KEY}\n")

        response = requests.post(TRADE_ENGINE_API_URL, headers=headers, json=signal_data, timeout=5)
        response.raise_for_status()
        log_stream.write(f"    [INFO] Signal sent to Trade Engine: {signal_data.get('pair_name', 'N/A')} {signal_data.get('action', 'N/A')}. Response: {response.json()}\n")
        return True
    except requests.exceptions.Timeout:
        log_stream.write(f"    [ERROR] Sending signal to Trade Engine timed out for {signal_data.get('pair_name', 'N/A')}.\n")
    except requests.exceptions.ConnectionError as e:
        log_stream.write(f"    [ERROR] Connection error sending signal to Trade Engine for {signal_data.get('pair_name', 'N/A')}: {e}. Is Trade Engine running?\n")
    except requests.exceptions.HTTPError as e:
        log_stream.write(f"    [ERROR] HTTP error sending signal to Trade Engine for {signal_data.get('pair_name', 'N/A')}: {e}. Response: {response.text}\n")
    except Exception as e:
        log_stream.write(f"    [ERROR] Unexpected error sending signal to Trade Engine for {signal_data.get('pair_name', 'N/A')}: {e}\n")
    return False

def start_realtime_monitoring(
    total_duration_minutes,
    interval_seconds,
    confidence_level=0.95,
    pipeline_run_id: Optional[str] = None,
    log_output_path: Optional[str] = None
):
    from statsmodels.tsa.statespace.varmax import VARMAXResultsWrapper
    from statsmodels.tsa.statespace.sarimax import SARIMAXResultsWrapper
    import numpy as np
    import os

    all_monitoring_results = []
    start_time = time.time()
    end_time = start_time + total_duration_minutes * 60
    cycle_count = 0

    if log_output_path:
        log_stream_main = open(log_output_path, 'a')
    else:
        log_stream_main = sys.stdout

    log_stream_main.write(f"\n[INFO] Starting real-time monitoring for {total_duration_minutes} minutes, checking every {interval_seconds} seconds.\n")
    log_stream_main.flush()

    # Initialize MetaTrader5 connection
    log_stream_main.write("[INFO] Initializing MetaTrader5...\n")
    mt5_adapter_instance = MT5Adapter(logger=lambda msg: log_stream_main.write(msg + '\n'))

    if not mt5_adapter_instance.initialize():
        log_stream_main.write(f"[ERROR] MetaTrader5 initialization failed, error code = {mt5_adapter_instance.last_error()}\n")
        log_stream_main.flush()
        if log_output_path: log_stream_main.close()
        mt5_adapter_instance.shutdown()
        return [], "MetaTrader5 initialization failed."

    if not mt5_adapter_instance.login(
        login=MT5_LOGIN,
        password=MT5_PASSWORD,
        server=MT5_SERVER
    ):
        log_stream_main.write(f"[ERROR] MetaTrader5 login failed, error code = {mt5_adapter_instance.last_error()}\n")
        log_stream_main.flush()
        if log_output_path: log_stream_main.close()
        mt5_adapter_instance.shutdown()
        return [], "MetaTrader5 login failed."

    log_stream_main.write("[OK] MetaTrader5 initialized and logged in successfully.\n")
    log_stream_main.flush()


    PAIR_TO_RLS_GROUP = {
       endog.replace("_Close_Log_Return", ""): group
       for group, endogs in parameter.VARX_ENDOG_GROUPS.items()
       for endog in endogs
       if endog.endswith("_Close_Log_Return")
   }

    # Define MT5_TIMEFRAME_MAP using the adapter's properties for fetch_high_frequency_data
    current_mt5_timeframe_map = {
        "1m": mt5_adapter_instance.TIMEFRAME_M1,
        "5m": mt5_adapter_instance.TIMEFRAME_M5,
        "15m": mt5_adapter_instance.TIMEFRAME_M15,
        "30m": mt5_adapter_instance.TIMEFRAME_M30,
        "1h": mt5_adapter_instance.TIMEFRAME_H1,
        "4h": mt5_adapter_instance.TIMEFRAME_H4,
        "1d": mt5_adapter_instance.TIMEFRAME_D1,
        "1w": mt5_adapter_instance.TIMEFRAME_W1,
        "1M": mt5_adapter_instance.TIMEFRAME_MN1,
    }

    pipeline_run_id_for_monitor = pipeline_run_id if pipeline_run_id is not None else "UNKNOWN_RUN_ID"

    from news_manager import NewsManager
    logger_news = lambda msg: (log_stream_main.write(f"[NEWS] {msg}\n"), log_stream_main.flush())
    news_manager_instance = NewsManager(data_dir=VPS_DATA_DIR, logger=logger_news)
    # Sync news to populate data, then load it
    logger_news("Attempting to sync news...")
    news_manager_instance.sync_news()
    logger_news("Loading local news...")
    news_manager_instance.load_local_news()

    restored_price_forecasts_with_intervals = {}
    final_stationarized_fred_data = {}
    fitted_varx_models = {}
    PAIR_TARGET_INDEX = {}
    for group, endogs in parameter.VARX_ENDOG_GROUPS.items():
        for idx, pair in enumerate(endogs):
            PAIR_TARGET_INDEX[pair] = idx

    log_stream_main.write(f"[INFO] Attempting to load data from VPS_DATA_DIR: {VPS_DATA_DIR}\n")
    log_stream_main.flush()

    try:
        forecast_path = os.path.join(VPS_DATA_DIR, os.path.basename(parameter.FORECAST_OUTPUT_PATH))
        with open(forecast_path, 'rb') as f:
            loaded_data = pickle.load(f)
            restored_price_forecasts_with_intervals = loaded_data.get("data", {})
        log_stream_main.write(f"[OK] Successfully loaded restored_price_forecasts_with_intervals from {forecast_path}\n")
    except FileNotFoundError:
        log_stream_main.write(f"[WARN] Forecast data file not found at {forecast_path}. Monitoring will be incomplete.\n")
    except Exception as e:
        log_stream_main.write(f"[ERROR] Failed to load restored_price_forecasts_with_intervals from {forecast_path}: {e}\n")
    log_stream_main.flush()

    try:
        fred_path = os.path.join(VPS_DATA_DIR, os.path.basename(parameter.FRED_DATA_PATH))
        with open(fred_path, 'rb') as f:
            loaded_data = pickle.load(f)
            final_stationarized_fred_data = loaded_data.get("data", {})
        log_stream_main.write(f"[OK] Successfully loaded final_stationarized_fred_data from {fred_path}\n")
    except FileNotFoundError:
        log_stream_main.write(f"[WARN] FRED data file not found at {fred_path}. Monitoring will be incomplete.\n")
    except Exception as e:
        log_stream_main.write(f"[ERROR] Failed to load final_stationarized_fred_data from {fred_path}: {e}\n")
    log_stream_main.flush()

    try:
        models_path = os.path.join(VPS_DATA_DIR, os.path.basename(parameter.FITTED_MODELS_PATH))
        with open(models_path, 'rb') as f:
            loaded_data = pickle.load(f)
            fitted_varx_models = loaded_data.get("data", {})
        log_stream_main.write(f"[OK] Successfully loaded fitted_varx_models from {models_path}\n")
    except FileNotFoundError:
        log_stream_main.write(f"[WARN] Fitted models file not found at {models_path}. RLS initialization might be incomplete.\n")
    except Exception as e:
        log_stream_main.write(f"[ERROR] Failed to load fitted_varx_models from {models_path}: {e}\n")
    log_stream_main.flush()

    rls_estimators: Dict[str, Dict[str, Any]] = {}
    if fitted_varx_models:
        log_stream_main.write(f"[INFO] Initializing RLS estimators for each VARX model group.\n")
        log_stream_main.flush()
        for group_name, model_res in fitted_varx_models.items():
            fitted_model_obj = model_res['fitted_model']
            endog_names_group = model_res['endog_names']
            exog_names_group = model_res.get('exog_names', [])

            n_endog_group = len(endog_names_group)

            if isinstance(fitted_model_obj, (VARMAXResultsWrapper, SARIMAXResultsWrapper)):
                try:
                    k_regressors = 1 + (parameter.maxlag_test * n_endog_group) + len(exog_names_group)
                    baseline_theta_ref = _extract_baseline_varx_params(log_stream_main, fitted_model_obj, n_endog_group, k_regressors, endog_names_group, exog_names_group, parameter.maxlag_test)

                    if baseline_theta_ref is None:
                        log_stream_main.write(f"  [WARN] Failed to extract baseline parameters for group {group_name}. Skipping RLS init.\n")
                        log_stream_main.flush()
                        continue

                    initial_theta = baseline_theta_ref
                    initial_P = parameter.RLS_INITIAL_P_DIAG * np.eye(k_regressors)

                    rls_estimators[group_name] = {
                        'theta': initial_theta,
                        'P': initial_P,
                        'baseline_theta_ref': baseline_theta_ref,
                        'n_endog': n_endog_group,
                        'k_regressors': k_regressors,
                        'endog_names': endog_names_group,
                        'exog_names': exog_names_group,
                        'maxlags': parameter.maxlag_test,
                        'rls_update_count': 0,
                        'pred_variance_history': []
                    }
                    log_stream_main.write(f"  [OK] RLS initialized for group {group_name}. Theta shape: {initial_theta.shape}, P shape: {initial_P.shape}\n")
                    log_stream_main.flush()

                except Exception as e:
                    log_stream_main.write(f"  [ERROR] Failed to initialize RLS for group {group_name}: {e}\n")
                    log_stream_main.flush()
            else:
                log_stream_main.write(f"  [WARN] Model type for {group_name} not recognized for RLS initialization.\n")
                log_stream_main.flush()
    else:
        log_stream_main.write(f"[WARN] No fitted VARX models provided for RLS initialization.\n")
        log_stream_main.flush()

    if not restored_price_forecasts_with_intervals or not fitted_varx_models:
        log_stream_main.write("[ERROR] Critical data or models missing. Cannot proceed with monitoring. Please ensure files are transferred correctly.\n")
        log_stream_main.flush()
        if log_output_path: log_stream_main.close()
        mt5_adapter_instance.shutdown()
        return [], "Critical data missing, monitoring aborted."

    try:
        while time.time() < end_time:
            cycle_count += 1
            cycle_start_time = time.time()
            log_stream = log_stream_main

            skip_individual_trade_decisions = False

            log_stream.write(f"\n--- Monitoring Cycle {cycle_count} (Started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')})---\n")
            log_stream.flush()

            hf_raw_data_dfs = fetch_high_frequency_data(
                log_stream,
                mt5_adapter_instance,
                current_mt5_timeframe_map,
                parameter.PAIRS,
                parameter.HF_LOOKBACK_DAYS,
                parameter.HF_BASE_INTERVAL
            )

            hf_log_returns_dict, hf_combined_log_returns_df, _, hf_fred_exog_aligned = preprocess_high_frequency_data(
                log_stream,
                hf_raw_data_dfs,
                _apply_log_return_to_price,
                _combine_log_returns,
                _test_and_stationarize_data,
                prepare_high_frequency_exogenous_data,
                final_stationarized_fred_data,
                parameter.alpha
            )

            if hf_combined_log_returns_df.empty or len(hf_combined_log_returns_df) <= parameter.maxlag_test:
                log_stream.write(f"  [WARN] Not enough valid high-frequency log returns for RLS. Skipping this cycle.\n")
                log_stream.flush()
                current_cycle_results_summary = {
                    "cycle_number": cycle_count,
                    "timestamp": datetime.now().isoformat(),
                    "latest_actual_prices": {},
                    "deviation_results": {},
                    "rls_forecast" : {},
                    "rls_health" : {},
                    "trade_signals": {},
                    "parameter_deviations": {},
                    "pipeline_run_id": pipeline_run_id_for_monitor,
                    "cycle_log": "Not enough data for RLS."
                }

                send_monitoring_data_to_colab(current_cycle_results_summary, log_stream)

                time_to_sleep = interval_seconds - (time.time() - cycle_start_time)
                if time_to_sleep > 0:
                    time.sleep(time_to_sleep)
                continue

            latest_hf_actual_prices = {}
            latest_hf_atrs = {}
            if hf_raw_data_dfs:
                for pair_name, df in hf_raw_data_dfs.items():
                    if not df.empty and 'Close' in df.columns:
                        latest_hf_actual_prices[pair_name] = df['Close'].iloc[-1]
                        atr_series = calculate_atr(log_stream, df)
                        if not atr_series.empty:
                            latest_hf_atrs[pair_name] = atr_series.iloc[-1]

            rls_forecasts = {}
            rls_metrics = {}
            parameter_deviations = {}
            confidence_per_group = {}
            rls_param_deviation_score = 0.0
            for group_name, estimator_data in rls_estimators.items():
                current_theta = estimator_data['theta']
                current_P = estimator_data['P']
                baseline_theta_ref = estimator_data['baseline_theta_ref']
                n_endog = estimator_data['n_endog']
                k_regressors = estimator_data['k_regressors']
                endog_names_group = estimator_data['endog_names']
                exog_names_group = estimator_data['exog_names']
                maxlags = estimator_data['maxlags']

                latest_hf_combined_log_returns_df_row = hf_combined_log_returns_df.iloc[[-1]]
                latest_hf_fred_exog_df_row = hf_fred_exog_aligned.iloc[[-1]] if not hf_fred_exog_aligned.empty else pd.DataFrame()

                lagged_data_for_phi_dict = {}

                for lag in range(1, maxlags + 1):
                    for endog_name in endog_names_group:
                        col_name = f'Lag{lag}_{endog_name}' 
                        try:
                            val = hf_combined_log_returns_df[endog_name].iloc[-lag]
                        except IndexError:
                            val = hf_combined_log_returns_df[endog_name].iloc[-1]
                        lagged_data_for_phi_dict[col_name] = val

                lagged_hf_log_returns_df = pd.DataFrame([lagged_data_for_phi_dict])


                Y_t = latest_hf_combined_log_returns_df_row[endog_names_group].values

                if Y_t.shape[0] == 0:
                    log_stream.write(f"    [WARN] No current endogenous data (Y_t) for RLS update for group {group_name}. Skipping RLS update.\n")
                    log_stream.flush()
                    continue

                Phi = _build_regressor_matrix(log_stream, latest_hf_combined_log_returns_df_row, latest_hf_fred_exog_df_row, lagged_hf_log_returns_df, maxlags, endog_names_group, exog_names_group)

                updated_theta, updated_P = _perform_rls_update(log_stream, current_theta, current_P, Phi, Y_t, parameter.FORGETTING_FACTOR)

                estimator_data['theta'] = updated_theta
                estimator_data['P'] = updated_P

                estimator_data["rls_update_count"] += 1
                n_rls_updates = estimator_data["rls_update_count"]

                try:
                    pred_variance = float(Phi @ updated_P @ Phi.T)
                except Exception:
                    pred_variance = float("inf")

                pred_variance = max(pred_variance, 1e-12)
                estimator_data["pred_variance_history"].append(pred_variance)
                deviation_norm = np.linalg.norm(updated_theta - baseline_theta_ref)

                min_updates = parameter.RLS_MIN_UPDATES_FOR_CONFIDENCE

                maturity = min(1.0, n_rls_updates / min_updates)

                if maturity < 1.0 or len(estimator_data["pred_variance_history"]) < 10:
                    confidence = 0.0
                else:
                   # 1. Gunakan rolling window untuk median (misal 60 data terakhir)
                    window_size = 60
                    recent_variance_history = list(estimator_data["pred_variance_history"])[-window_size:]
                    variance_ref = np.median(recent_variance_history) if recent_variance_history else 1e-12

                    # 2. Hitung ketidakpastian relatif terhadap volatilitas terkini
                    normalized_uncertainty = pred_variance / (variance_ref + 1e-12)

                    # 3. Hitung penalti deviasi (skor deviasi yang kita perbaiki tadi)
                    # Jika deviasi sangat tinggi, confidence harus turun secara linear/eksponensial
                    dev_penalty = np.exp(-0.2 * deviation_norm) # Semakin besar deviasi, semakin kecil multiplier

                    # 4. Gabungkan semuanya
                    confidence = float(
                        np.clip(
                            maturity * dev_penalty * np.exp(-parameter.RLS_CONFIDENCE_ALPHA * normalized_uncertainty),
                            0.0,
                            1.0
                        )
                    )

                rls_metrics[group_name] = {
                    "confidence": float(confidence),
                    "maturity": float(maturity),
                    "deviation": float(deviation_norm),
                    "pred_var": float(pred_variance)
                }
                confidence_per_group[group_name] = confidence
                parameter_deviations[group_name] = float(deviation_norm)

                log_stream.write(
                    f"    [INFO] {group_name} | "
                    f"Deviation: {deviation_norm:.4f} | "
                    f"Confidence: {confidence:.3f} | "
                    f"Maturity: {maturity:.2f} | "
                    f"PredVar: {pred_variance:.6e}\n"
                )
                log_stream.flush()

            if parameter_deviations:
                all_deviations = list(parameter_deviations.values())
                rls_param_deviation_score = np.mean(all_deviations)
                global_rls_confidence = np.mean(list(confidence_per_group.values()))
            else:
                rls_param_deviation_score = 0.0
                global_rls_confidence = 0.0

            log_stream.write(f"    [INFO] GLOBAL RLS SCORE | Deviation: {rls_param_deviation_score:.4f} | Confidence: {global_rls_confidence:.3f}\n")
            log_stream.flush()

            if global_rls_confidence < parameter.RLS_CONFIDENCE_ENTRY_THRESHOLD:
                skip_individual_trade_decisions = True
                log_stream.write(
                    f"    [WARN] Global RLS Confidence ({global_rls_confidence:.4f}) "
                    f"is below Entry Threshold ({parameter.RLS_CONFIDENCE_ENTRY_THRESHOLD:.4f}). "
                    f"New trade entries are PAUSED for this cycle.\n"
                )
            else:
                log_stream.write(
                    f"    [INFO] Global RLS Confidence ({global_rls_confidence:.4f}) is OK. "
                )

            if rls_param_deviation_score > parameter.RLS_DEVIATION_CLOSE_ALL_THRESHOLD:
                log_stream.write(f"\n    [ALERT] RLS parameter deviation ({rls_param_deviation_score:.4f}) exceeds GLOBAL CLOSE ALL threshold ({parameter.RLS_DEVIATION_CLOSE_ALL_THRESHOLD:.4f}). Sending signal to close all open positions.\n")
                send_signal_to_trade_engine({"signal_id": f"CLOSE_ALL_RISK_{pipeline_run_id_for_monitor}_{cycle_count}", "action": "CLOSE_ALL"}, log_stream)
                skip_individual_trade_decisions = True
                log_stream.flush()
            else:
                log_stream.write(f"    [INFO] Global RLS deviation ({rls_param_deviation_score:.4f}) is below CLOSE ALL threshold.\n")
                log_stream.flush()

            deviation_results = detect_price_deviation(
                log_stream,
                latest_hf_actual_prices,
                restored_price_forecasts_with_intervals,
                confidence_level
            )

            if news_manager_instance.is_currently_restricted():
                skip_individual_trade_decisions = True
                log_stream.write(f"    [WARN] News restriction detected. Setting skip_individual_trade_decisions to True.\n")
                log_stream.flush()

            trade_signals = {}

            if not skip_individual_trade_decisions:
                for pair_name, forecast_data in restored_price_forecasts_with_intervals.items():

                    if pair_name not in latest_hf_actual_prices or pair_name not in latest_hf_atrs:
                        log_stream.write(
                            f"    [WARN] Skipping trade decision for {pair_name}: Missing latest actual price or ATR.\n"
                        )
                        trade_signals[pair_name] = {
                            "signal": "HOLD",
                            "entry_price": np.nan,
                            "stop_loss": np.nan,
                            "take_profit": np.nan,
                            "position_units": 0,
                            "rr_ratio": np.nan,
                            "snr": np.nan,
                            "reason": "Missing high-frequency data"
                        }
                        continue

                    first_forecast_step = forecast_data.iloc[0]
                    predicted_mean = first_forecast_step["Close_Mean_Forecast"]
                 
                    z_score_value = norm.ppf(1 - (1 - confidence_level) / 2)
                    upper_ci = first_forecast_step[f"Close_Upper_{int(confidence_level*100)}CI"]
                    forecast_std = (upper_ci - predicted_mean) / z_score_value if z_score_value != 0 else np.nan
                    steps_in_period = 96 # D1 ke M15 = (24 jam * 60 menit) / 15 menit = 96
                    forecast_std_price_per_bar = forecast_std / np.sqrt(steps_in_period)
                    forecast_std_return = forecast_std_price_per_bar / latest_hf_actual_prices[pair_name] # forecast_std is PRICE std; converted to RETURN only for SNR

                    pair_group = PAIR_TO_RLS_GROUP.get(pair_name)

                    if pair_group is None:
                        log_stream.write(
                            f"    [WARN] {pair_name}: No RLS group mapping found via VARX_ENDOG_GROUPS. "
                            f"Skipping trade for safety.\n"
                        )

                        trade_signals[pair_name] = {
                            "signal": "HOLD",
                            "entry_price": np.nan,
                            "stop_loss": np.nan,
                            "take_profit": np.nan,
                            "position_units": 0,
                            "rr_ratio": np.nan,
                            "snr": np.nan,
                            "reason": "No RLS group mapping"
                        }
                        continue
                    
                    rls_expected_return = infer_rls_expected_return(
                        log_stream=log_stream,
                        pair_name=pair_name, # Fungsi akan mencari group & index sendiri
                        rls_estimators=rls_estimators,
                        current_hf_combined_log_returns_df=latest_hf_combined_log_returns_df_row,
                        latest_hf_fred_exog_df=latest_hf_fred_exog_df_row,
                        lagged_hf_log_returns_df=lagged_hf_log_returns_df
                    )

                    rls_predicted_price = latest_hf_actual_prices[pair_name] * np.exp(rls_expected_return)

                    rls_forecasts[pair_name] = {
                        "rls_predicted_price": float(rls_predicted_price),
                        "rls_expected_return_pct": float(rls_expected_return * 100)
                    }

                    if rls_expected_return is None:
                        trade_signals[pair_name] = HOLD_REASON("RLS unavailable")
                        continue

                    pair_rls_deviation = parameter_deviations.get(pair_group, float("inf"))

                    if pair_rls_deviation > parameter.RLS_DEVIATION_THRESHOLD:
                        log_stream.write(
                            f"    [WARN] {pair_name}: RLS deviation for group "
                            f"{pair_group} ({pair_rls_deviation:.4f}) exceeds threshold "
                            f"({parameter.RLS_DEVIATION_THRESHOLD:.4f}). "
                            f"Model parameters are unstable.\n"
                        )
                        trade_signals[pair_name] = {
                            "signal": "HOLD",
                            "entry_price": latest_hf_actual_prices[pair_name],
                            "stop_loss": np.nan,
                            "take_profit": np.nan,
                            "position_units": 0,
                            "rr_ratio": np.nan,
                            "snr": np.nan,
                            "reason": f"RLS deviation too high for group {pair_group}"
                        }
                        continue
                    account_info = mt5_adapter_instance.account_info()
                    if account_info is not None:
                        current_equity = account_info.equity
                        # Gunakan balance jika ingin risiko lebih konservatif saat ada floating loss
                        # current_balance = account_info.balance 
                    else:
                        log_stream.write("[ERROR] Could not get account info, using fallback equity.\n")
                        current_equity = parameter.EQUITY # Fallback

                    signal = decide_trade(
                        log_stream=log_stream,
                        pair_name=pair_name,
                        latest_actual_price=latest_hf_actual_prices[pair_name],
                        expected_return=rls_expected_return,
                        forecast_std=forecast_std,          # Masukkan versi PRICE di sini
                        forecast_std_return=forecast_std_return, # Masukkan versi RETURN di sini
                        hf_atr=latest_hf_atrs[pair_name],
                        equity=current_equity,
                        risk_pct=parameter.RISK_PER_TRADE_PCT,
                        k_atr_stop=parameter.K_ATR_STOP,
                        k_model_stop=parameter.K_MODEL_STOP,
                        snr_threshold=parameter.SNR_THRESHOLD,
                        rls_param_deviation_score=pair_rls_deviation,
                        rls_deviation_threshold=parameter.RLS_DEVIATION_THRESHOLD,
                        tp_rr_ratio=parameter.TP_RR_RATIO
                    )

                    trade_signals[pair_name] = signal

                    # ✅ KIRIM DI SINI
                    if signal["signal"] in ("BUY", "SELL"):
                        log_stream.write(
                            f"  [INFO] Sending {signal['signal']} signal for {pair_name} to Trade Engine...\n"
                        )
                        send_signal_to_trade_engine(
                            {
                                "signal_id": f"{pair_name.replace('/', '')}",
                                "action": signal["signal"],
                                "symbol": pair_name,
                                "entry_price": signal["entry_price"],
                                "stop_loss": signal["stop_loss"],
                                "take_profit": signal["take_profit"],
                                "position_units": signal["position_units"],
                                "snr": signal.get("snr"),
                                "pipeline_run_id": pipeline_run_id_for_monitor,
                            },
                            log_stream,
                        )

            else:
                # Logika penentu alasan skip untuk transparansi audit
                reasons = []
                if global_rls_confidence < parameter.RLS_CONFIDENCE_ENTRY_THRESHOLD:
                    reasons.append(f"Low Confidence ({global_rls_confidence:.4f})")
                if rls_param_deviation_score > parameter.RLS_DEVIATION_CLOSE_ALL_THRESHOLD:
                    reasons.append(f"High Deviation ({rls_param_deviation_score:.4f})")
                if news_manager_instance.is_currently_restricted():
                    reasons.append("News Restriction")

                reason_str = ", ".join(reasons) if reasons else "Unknown Safety Filter"

                log_stream_main.write(
                    f"    [INFO] Skipping individual trade decisions. Reasons: [{reason_str}]\n"
                )

                for pair_name in restored_price_forecasts_with_intervals:
                    trade_signals[pair_name] = {
                        "signal": "HOLD",
                        "entry_price": latest_hf_actual_prices.get(pair_name, np.nan),
                        "stop_loss": np.nan,
                        "take_profit": np.nan,
                        "position_units": 0,
                        "rr_ratio": np.nan,
                        "snr": np.nan,
                        "reason": "Skipped due to global RLS deviation threshold breach",
                    }


            current_cycle_results = {
                "cycle_number": cycle_count,
                "timestamp": datetime.now().isoformat(),
                "latest_actual_prices": convert_numpy_floats(latest_hf_actual_prices),
                "rls_health": convert_numpy_floats(rls_metrics),
                "rls_forecast": format_for_dashboard(rls_forecasts, latest_hf_actual_prices),
                "deviation_results": convert_numpy_floats(deviation_results),
                "trade_signals": convert_numpy_floats(trade_signals),
                "parameter_deviations": convert_numpy_floats(parameter_deviations),
                "pipeline_run_id": pipeline_run_id_for_monitor,
                "cycle_duration_seconds": float(time.time() - cycle_start_time),
                "log_summary": f"Completed cycle {cycle_count}. Price deviation for {sum(1 for r in deviation_results.values() if r['ci_breach'])} pairs. Trade signals generated for {sum(1 for s in trade_signals.values() if s['signal'] != 'HOLD')} pairs."
            }

            # --- Position Modification Logic (NEW) ---
            log_stream.write(f"\n[INFO] Checking for position modifications...\n")
            if mt5_adapter_instance._logged_in:
                open_positions = mt5_adapter_instance.positions_get(magic=parameter.MAGIC_NUMBER)
                if open_positions:
                    log_stream.write(f"  [INFO] Found {len(open_positions)} open positions to consider for modification.\n")
                    for pos in open_positions:
                        pos_symbol = pos.symbol
                        pos_ticket = pos.ticket
                        pos_type = pos.type
                        current_sl = pos.sl
                        current_tp = pos.tp
                        pos_open_price = pos.price_open

                        mapped_pair_name = None
                        for p_name, yf_symbol in parameter.PAIRS.items():
                            if pos_symbol.replace("/", "").replace("=X", "") == yf_symbol.replace("=X", "").replace("-", "").replace("/", ""):
                                mapped_pair_name = p_name
                                break
                        if mapped_pair_name is None:
                            log_stream.write(f"    [WARN] Could not map MT5 symbol '{pos_symbol}' to a known pair name. Skipping modification for ticket {pos_ticket}.\n")
                            continue

                        if mapped_pair_name not in restored_price_forecasts_with_intervals or \
                           mapped_pair_name not in latest_hf_actual_prices or \
                           mapped_pair_name not in latest_hf_atrs:
                            log_stream.write(f"    [WARN] Missing forecast/price/ATR data for {mapped_pair_name}. Skipping modification for ticket {pos_ticket}.\n")
                            continue

                        forecast_data_for_pair = restored_price_forecasts_with_intervals[mapped_pair_name]
                        if forecast_data_for_pair.empty:
                            log_stream.write(f"    [WARN] Forecast data is empty for {mapped_pair_name}. Skipping ticket {pos_ticket}.\n")
                            continue
                        
                        # 3. Ekstraksi Parameter Forecast
                        first_forecast_step = forecast_data_for_pair.iloc[0]
                        predicted_mean = first_forecast_step[f'Close_Mean_Forecast']
                        z_score_value = norm.ppf(1 - (1 - confidence_level) / 2)
                        upper_ci = first_forecast_step[f'Close_Upper_{int(confidence_level*100)}CI']
                        forecast_std = (upper_ci - predicted_mean) / z_score_value if z_score_value != 0 else np.nan

                        if np.isnan(forecast_std) or forecast_std <= 0:
                            log_stream.write(f"    [WARN] Invalid forecast standard deviation for {mapped_pair_name}. Skipping ticket {pos_ticket}.\n")
                            continue

                        is_buy = pos_type == mt5_adapter_instance.ORDER_TYPE_BUY
                        is_sell = pos_type == mt5_adapter_instance.ORDER_TYPE_SELL

                        # 4. Inferensi RLS Expected Return (Automated Group Search)
                        rls_expected_return = infer_rls_expected_return(
                            log_stream=log_stream,
                            pair_name=mapped_pair_name,
                            rls_estimators=rls_estimators,
                            current_hf_combined_log_returns_df=latest_hf_combined_log_returns_df_row,
                            latest_hf_fred_exog_df=latest_hf_fred_exog_df_row,
                            lagged_hf_log_returns_df=lagged_hf_log_returns_df
                        )

                        if rls_expected_return is None:
                            log_stream.write(f"    [WARN] RLS expected return unavailable for {mapped_pair_name}. Skipping flip check for {pos_ticket}.\n")
                            continue

                        # 5. Logika RLS Direction Flip (Exit Early)
                        latest_actual_price = latest_hf_actual_prices.get(mapped_pair_name)
                        hf_atr = latest_hf_atrs.get(mapped_pair_name)
                        RLS_FLIP_EPS = forecast_std_return * 1  # Gunakan 10% dari volatilitas model

                        close_due_to_rls_flip = (
                            (is_buy and rls_expected_return < -RLS_FLIP_EPS) or
                            (is_sell and rls_expected_return > RLS_FLIP_EPS)
                        )

                        if close_due_to_rls_flip:
                            log_stream.write(f"    [ALERT] Closing position {pos_ticket} ({mapped_pair_name}) due to RLS flip. μ={rls_expected_return:+.6f}\n")
                            close_signal = {
                                "signal_id": f"CLOSE_RLS_FLIP_{pos_ticket}_{cycle_count}",
                                "action": "CLOSE",
                                "ticket": pos_ticket,
                                "symbol": mapped_pair_name,
                                "reason": "RLS direction flip"
                            }
                            send_signal_to_trade_engine(close_signal, log_stream)
                            continue 

                        # 6. Penentuan pair_group untuk SL/TP Adjustments
                        # Dicari ulang dari estimators untuk menjaga konsistensi state
                        pair_group = next((g for g, est in rls_estimators.items() 
                                          if any(mapped_pair_name in n for n in est['endog_names'])), None)

                        # 7. Dynamic SL/TP Adjustments berdasarkan RLS Deviation Score
                        k_atr_stop_adjusted = parameter.K_ATR_STOP
                        k_model_stop_adjusted = parameter.K_MODEL_STOP
                        tp_rr_ratio_adjusted = parameter.TP_RR_RATIO

                        if pair_group and rls_param_deviation_score is not None and not np.isnan(rls_param_deviation_score):
                            pair_rls_deviation = parameter_deviations.get(pair_group, 0.0)
                            increase_factor_sl = 1 + pair_rls_deviation * parameter.RLS_SCALING_FACTOR_SL
                            
                            k_atr_stop_adjusted = min(parameter.K_ATR_STOP * increase_factor_sl, 
                                                      parameter.K_ATR_STOP * parameter.RLS_SL_MAX_MULTIPLIER)
                            k_model_stop_adjusted = min(parameter.K_MODEL_STOP * increase_factor_sl, 
                                                        parameter.K_MODEL_STOP * parameter.RLS_SL_MAX_MULTIPLIER)

                            reduction_factor_tp = 1 - (pair_rls_deviation * parameter.RLS_SCALING_FACTOR_TP)
                            tp_rr_ratio_adjusted = max(parameter.RLS_TP_RR_MIN, tp_rr_ratio_adjusted * reduction_factor_tp)
                            
                            log_stream.write(f"    [INFO] Dynamic adjustments: k_atr={k_atr_stop_adjusted:.2f}, k_model={k_model_stop_adjusted:.2f}, TP_RR={tp_rr_ratio_adjusted:.2f}\n")

                        # 8. Kalkulasi Jarak SL/TP Baru
                        predicted_mean_rls = latest_actual_price * np.exp(rls_expected_return)
                        sl_dist_atr = k_atr_stop_adjusted * hf_atr
                        sl_dist_model = (k_model_stop_adjusted * forecast_std_return * latest_actual_price)
                        sl_dist = max(sl_dist_atr, sl_dist_model)

                        if sl_dist <= 1e-9:
                            log_stream.write(f"    [WARN] SL Distance too small for {mapped_pair_name} ({pos_ticket}). Skipping.\n")
                            continue

                        # A. SL Baru: One-Way Trailing (Hanya bergerak mendekati harga profit)
                        if is_buy:
                            target_sl = latest_actual_price - sl_dist
                            # SL hanya boleh naik (max)
                            new_sl = max(target_sl, current_sl) if current_sl != 0 else target_sl
                            # TP dihitung dari harga OPEN (Fixed Target)
                            new_tp = predicted_mean_rls
                        else:
                            target_sl = latest_actual_price + sl_dist
                            # SL hanya boleh turun (min)
                            new_sl = min(target_sl, current_sl) if current_sl != 0 else target_sl
                            # TP dihitung dari harga OPEN (Fixed Target)
                            new_tp = predicted_mean_rls
                        if new_sl < 0 or new_tp < 0:
                             log_stream.write(f"    [WARN] Non-positive SL/TP for {mapped_pair_name} ({pos_ticket}). Skipping.\n")
                             continue

                        # 9. Verifikasi Threshold Modifikasi (Point Checking)
                        symbol_info_for_pos = mt5_adapter_instance.symbol_info(pos_symbol)
                        if symbol_info_for_pos is None:
                            continue
                        
                        min_price_change = symbol_info_for_pos.point * 10
                        if abs(new_sl - current_sl) > min_price_change or abs(new_tp - current_tp) > min_price_change:
                            log_stream.write(f"    [INFO] Sending MODIFY Ticket {pos_ticket} ({mapped_pair_name}). New SL: {new_sl:.4f}, TP: {new_tp:.4f}\n")
                            modify_signal = {
                                "signal_id": f"MODIFY_SLTP_{pos_ticket}_{cycle_count}",
                                "action": "MODIFY",
                                "ticket": pos_ticket,
                                "symbol": mapped_pair_name,
                                "new_sl": new_sl,
                                "new_tp": new_tp
                            }
                            send_signal_to_trade_engine(modify_signal, log_stream)
                        else:
                            log_stream.write(f"    [INFO] Ticket {pos_ticket}: Change below threshold. No action.\n")
                else:
                    log_stream.write(f"  [INFO] No open positions found for modification.\n")
            else:
                log_stream.write(f"  [INFO] Skipping position modification check (MT5 Offline or Global Pause).\n")
                
            current_cycle_results_summary = {
                "cycle_number": cycle_count,
                "timestamp": datetime.now().isoformat(),
                "latest_actual_prices": convert_numpy_floats(latest_hf_actual_prices),
                "rls_health": convert_numpy_floats(rls_metrics), # Gunakan rls_metrics yang diisi di loop
                "deviation_results": convert_numpy_floats(deviation_results),
                "rls_forecast": format_for_dashboard(rls_forecasts, latest_hf_actual_prices),
                #"rls_forecast": convert_numpy_floats(rls_forecasts),
                "trade_signals": convert_numpy_floats(trade_signals),
                "parameter_deviations": convert_numpy_floats(parameter_deviations),
                "pipeline_run_id": pipeline_run_id_for_monitor,
                "global_metrics": {
                    "global_confidence": float(global_rls_confidence),
                    "global_deviation": float(rls_param_deviation_score),
                    "cycle_duration": float(time.time() - cycle_start_time)
                }
            }
            all_monitoring_results.append(current_cycle_results_summary)

            send_monitoring_data_to_colab(current_cycle_results_summary, log_stream)

            cycle_duration = time.time() - cycle_start_time
            log_stream.write(f"--- Monitoring Cycle {cycle_count} (Finished: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}). Cycle Duration: {cycle_duration:.2f} seconds. Pipeline Run ID: {pipeline_run_id_for_monitor}---\n")
            log_stream.flush()

            time_to_sleep = interval_seconds - cycle_duration
            if time_to_sleep > 0:
                log_stream.write(f"    [INFO] Waiting for {time_to_sleep:.2f} seconds until next cycle.\n")
                time.sleep(time_to_sleep)
            else:
                log_stream.write(f"    [WARN] Cycle duration ({cycle_duration:.2f}s) exceeded interval ({interval_seconds}s). No sleep.\n")

    except Exception as e:
        log_stream.write(f"[CRITICAL ERROR] Monitoring loop encountered an unhandled exception: {e}\n")
        import traceback
        log_stream.write(traceback.format_exc())
        return all_monitoring_results, f"Critical error during monitoring: {e}"

    finally:
        log_stream.write("\n[INFO] Real-time monitoring finished.\n")
        log_stream.flush()
        if log_output_path: log_stream_main.close()
        mt5_adapter_instance.shutdown()

    return all_monitoring_results, "Monitoring completed successfully."
